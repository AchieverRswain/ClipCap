# ClipCap

# AI-Powered Image Caption Generator

##  Overview
This project is an advanced **AI-powered image caption generator** that combines **CLIP** for image understanding and **GPT-2** for generating natural language captions. It takes an image as input and produces highly relevant and context-aware descriptions.




##Results images : 









## ✨ Features
-  **CLIP + GPT-2 Integration**: Combines vision and language models for high-quality captions.
-  **User-Friendly Interface**: Upload an image and receive captions instantly.
-  **Accurate & Contextual Captions**: Uses beam search and top-p sampling for optimized results.
-  **Fast & Efficient**: Optimized inference pipeline for quick responses.

## 📌 How It Works
1.  **Image Upload**: Users provide an image input.
2.  **Feature Extraction**: CLIP encodes the image into embeddings.
3.  **Projection & Text Generation**: A projector network maps embeddings into GPT-2’s input space, generating captions.
4.  **Caption Output**: The model returns a meaningful caption based on the image content.



## 🛠️ Tech Stack
- **Python** 
- **CLIP (Contrastive Language-Image Pretraining)** 
- **GPT-2 (Text Generation Model)** ✍


## 📊 Results
- Generated captions exhibit **high accuracy and contextual relevance**.
- Compared to traditional image captioning methods, **this approach ensures diverse and fluent descriptions**.
- Evaluated on benchmark datasets with **strong qualitative and quantitative results**.

## 📜 Future Enhancements
- 🔄 Fine-tune GPT-2 for domain-specific captioning.
- 🌍 Deploy as a cloud-based API for wider accessibility.
- 🏆 Improve speed and scalability with optimized model inference.

## 🤝 Contributing
Feel free to fork the repository, create issues, or submit pull requests to improve the project.

## 📬 Contact
For any queries, reach out via email at `rohanswain2004@gmail.com` or connect on [LinkedIn](https://linkedin.com/in/rohanswain27).

---
⭐ **If you found this useful, don't forget to star the repo!** ⭐

